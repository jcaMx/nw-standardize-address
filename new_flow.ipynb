{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0a26270",
   "metadata": {},
   "source": [
    "## New Standardization Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b48d9dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cleaned_addresses.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('NW Publisher â€“ Address Export - 20250722 - complete.csv')\n",
    "\n",
    "# Filter and clean\n",
    "clean = df[df['Status'] != 'NewFromPublisher']\n",
    "\n",
    "# Deduplicate by Number + Street + PostalCode\n",
    "clean_unique = clean.drop_duplicates(subset=['Number','Street','PostalCode'], keep='first')\n",
    "\n",
    "# Save\n",
    "out = 'cleaned_addresses.csv'\n",
    "clean_unique.to_csv(out, index=False)\n",
    "out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4fc4ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] saved cache: 903052323120.json (HTTP 200)\n",
      "[1] saved cache: 903052323166.json (HTTP 200)\n",
      "[2] saved cache: 903052346170.json (HTTP 200)\n",
      "[3] saved cache: 903052346215.json (HTTP 200)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# --- CONFIGURE ---\n",
    "AUTH_ID = \"c158cf93-8a4d-c43c-dade-f372cbeac850\"   # replace if needed\n",
    "AUTH_TOKEN = \"e7UMwEf0GevqQpxwdLaO\"                # replace if needed\n",
    "SMARTY_URL = \"https://us-street.api.smarty.com/street-address\"\n",
    "\n",
    "INPUT_CSV = os.path.join(\"cleaned_sample.csv\")  # input file (expects Number/Street/Suburb/State/PostalCode or similar)\n",
    "CACHE_DIR = os.path.join(\"json_cache\")\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# candidate names for unit/apartment in input\n",
    "SECONDARY_CANDIDATES = [\n",
    "    \"ApartmentNumber\", \"Apartment\", \"Apt\", \"Unit\", \"Secondary\", \"SecondaryUnit\",\n",
    "    \"apt_number\", \"apt\", \"unit_number\"\n",
    "]\n",
    "\n",
    "def get_input_secondary(row):\n",
    "    for k in SECONDARY_CANDIDATES:\n",
    "        if k in row.index:\n",
    "            v = row.get(k)\n",
    "            if pd.notna(v) and str(v).strip():\n",
    "                return str(v).strip()\n",
    "    return \"\"\n",
    "\n",
    "def safe_filename(s):\n",
    "    return \"\".join(c if c.isalnum() or c in \"._- \" else \"_\" for c in str(s)).strip().replace(\" \", \"_\")\n",
    "\n",
    "def save_response(name_base, data, meta):\n",
    "    path = os.path.join(CACHE_DIR, f\"{name_base}.json\")\n",
    "    meta_path = os.path.join(CACHE_DIR, f\"{name_base}_meta.json\")\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "    for i, row in df.iterrows():\n",
    "        number = str(row.get(\"Number\", \"\")).strip()\n",
    "        street_raw = str(row.get(\"Street\", \"\")).strip()\n",
    "        street = f\"{number} {street_raw}\".strip()\n",
    "        city = str(row.get(\"Suburb\", \"\")).strip()\n",
    "        state = str(row.get(\"State\", \"CA\")).strip()\n",
    "        zipcode = str(row.get(\"PostalCode\", \"\")).replace(\".0\", \"\").strip()\n",
    "        input_secondary = get_input_secondary(row)\n",
    "\n",
    "        params = {\n",
    "            \"auth-id\": AUTH_ID,\n",
    "            \"auth-token\": AUTH_TOKEN,\n",
    "            \"street\": street,\n",
    "            \"city\": city,\n",
    "            \"state\": state,\n",
    "            \"zipcode\": zipcode,\n",
    "        }\n",
    "        if input_secondary:\n",
    "            params[\"secondary\"] = input_secondary\n",
    "\n",
    "        # best-effort filename check: if response cached by input row (index) skip\n",
    "        tentative_name = f\"row_{i}\"\n",
    "        meta_stub = {\"row_index\": int(i), \"sent_params\": params, \"input_secondary\": input_secondary, \"timestamp\": datetime.utcnow().isoformat() + \"Z\"}\n",
    "\n",
    "        # Do request\n",
    "        try:\n",
    "            r = requests.get(SMARTY_URL, params=params, timeout=15)\n",
    "            # try parse JSON even on non-200 to capture error responses in cache\n",
    "            data = r.json() if r.content else None\n",
    "            # compute name base from delivery_point_barcode if possible\n",
    "            name_base = tentative_name\n",
    "            if isinstance(data, list) and len(data) > 0 and isinstance(data[0], dict):\n",
    "                dpb = data[0].get(\"delivery_point_barcode\") or data[0].get(\"metadata\", {}).get(\"delivery_point_barcode\")\n",
    "                if dpb:\n",
    "                    name_base = safe_filename(dpb)\n",
    "                else:\n",
    "                    # try primary number + zipcode for nicer name\n",
    "                    comps = data[0].get(\"components\", {})\n",
    "                    pn = comps.get(\"primary_number\") or number or str(i)\n",
    "                    z = comps.get(\"zipcode\") or zipcode or \"\"\n",
    "                    name_base = safe_filename(f\"{pn}_{z}_{i}\")\n",
    "            else:\n",
    "                # fallback name includes street and index\n",
    "                name_base = safe_filename(f\"{street_raw}_{i}\")[:120] or tentative_name\n",
    "\n",
    "            save_response(name_base, data, meta_stub | {\"status_code\": r.status_code})\n",
    "            print(f\"[{i}] saved cache: {name_base}.json (HTTP {r.status_code})\")\n",
    "\n",
    "            # Respect rate limit a bit\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        except Exception as e:\n",
    "            # save error meta for debugging\n",
    "            err_meta = meta_stub | {\"error\": str(e)}\n",
    "            err_name = safe_filename(f\"error_row_{i}\")\n",
    "            with open(os.path.join(CACHE_DIR, f\"{err_name}_meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(err_meta, f, indent=2)\n",
    "            print(f\"[{i}] Exception: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8065442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved parsed cached data to: output\\cached_parsed_addresses.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "CACHE_DIR = os.path.join(\"json_cache\")\n",
    "OUT_CSV = os.path.join(\"output\", \"cached_parsed_addresses.csv\")\n",
    "os.makedirs(os.path.dirname(OUT_CSV), exist_ok=True)\n",
    "\n",
    "def parse_file(path):\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    # If data is a list, take first candidate (common Smarty response)\n",
    "    entry = data[0] if isinstance(data, list) and len(data) > 0 else (data if isinstance(data, dict) else None)\n",
    "    if not entry:\n",
    "        return None\n",
    "\n",
    "    comps = entry.get(\"components\", {})\n",
    "    meta = entry.get(\"metadata\", {})\n",
    "    analysis = entry.get(\"analysis\", {})\n",
    "    row = {\n",
    "        \"cache_file\": os.path.basename(path),\n",
    "        \"delivery_line_1\": entry.get(\"delivery_line_1\", \"\"),\n",
    "        \"delivery_line_2\": entry.get(\"delivery_line_2\", \"\"),\n",
    "        \"last_line\": entry.get(\"last_line\", \"\"),\n",
    "        \"delivery_point_barcode\": entry.get(\"delivery_point_barcode\", \"\") or meta.get(\"delivery_point_barcode\", \"\"),\n",
    "        # components\n",
    "        \"primary_number\": comps.get(\"primary_number\", \"\"),\n",
    "        \"street_predirection\": comps.get(\"street_predirection\", \"\"),\n",
    "        \"street_name\": comps.get(\"street_name\", \"\"),\n",
    "        \"street_suffix\": comps.get(\"street_suffix\", \"\"),\n",
    "        \"secondary_designator\": comps.get(\"secondary_designator\", \"\"),\n",
    "        \"secondary_number\": comps.get(\"secondary_number\", \"\"),\n",
    "        \"city_name\": comps.get(\"city_name\", \"\") or comps.get(\"default_city_name\", \"\"),\n",
    "        \"state_abbreviation\": comps.get(\"state_abbreviation\", \"\"),\n",
    "        \"zipcode\": comps.get(\"zipcode\", \"\"),\n",
    "        \"plus4_code\": comps.get(\"plus4_code\", \"\"),\n",
    "        \"delivery_point\": comps.get(\"delivery_point\", \"\"),\n",
    "        \"delivery_point_check_digit\": comps.get(\"delivery_point_check_digit\", \"\"),\n",
    "        \n",
    "        # metadata/location\n",
    "        \"latitude\": meta.get(\"latitude\", \"\"),\n",
    "        \"longitude\": meta.get(\"longitude\", \"\"),\n",
    "        \"precision\": meta.get(\"precision\", \"\"),\n",
    "        # analysis/dpv\n",
    "        \"dpv_match_code\": analysis.get(\"dpv_match_code\", \"\"),\n",
    "        \"dpv_footnotes\": analysis.get(\"dpv_footnotes\", \"\"),\n",
    "        \"active\": analysis.get(\"active\", \"\"),\n",
    "    }\n",
    "    return row\n",
    "\n",
    "def main():\n",
    "    files = sorted(glob(os.path.join(CACHE_DIR, \"*.json\")))\n",
    "    # ignore meta companion files if any (we included *_meta.json)\n",
    "    files = [f for f in files if not f.endswith(\"_meta.json\")]\n",
    "    rows = []\n",
    "    for fpath in files:\n",
    "        parsed = parse_file(fpath)\n",
    "        if parsed:\n",
    "            rows.append(parsed)\n",
    "    if not rows:\n",
    "        print(\"No parsed rows found in cache.\")\n",
    "        return\n",
    "    df = pd.DataFrame(rows)\n",
    "    # add a composed StandardizedAddress column using delivery lines when available\n",
    "    def compose_std(r):\n",
    "        parts = []\n",
    "        if r.get(\"delivery_line_1\"):\n",
    "            parts.append(r[\"delivery_line_1\"])\n",
    "        if r.get(\"delivery_line_2\"):\n",
    "            parts.append(r[\"delivery_line_2\"])\n",
    "        if r.get(\"last_line\"):\n",
    "            parts.append(r[\"last_line\"])\n",
    "        return \", \".join([p for p in parts if p])\n",
    "    df[\"StandardizedAddress\"] = df.apply(compose_std, axis=1)\n",
    "    df.to_csv(OUT_CSV, index=False)\n",
    "    print(\"Saved parsed cached data to:\", OUT_CSV)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0be4b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
